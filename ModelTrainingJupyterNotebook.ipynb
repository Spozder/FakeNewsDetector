{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow.keras as keras\n",
    "print(\"Tensorflow version: {}\".format(tf.__version__))\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants!\n",
    "DATASET_SIZE = 9408908\n",
    "TRAINING_SIZE = (DATASET_SIZE // 8) * 6\n",
    "TESTING_SIZE = DATASET_SIZE // 8\n",
    "VALIDATION_SIZE = DATASET_SIZE // 8\n",
    "BATCH_SIZE = 128\n",
    "SELECT_COLUMNS = ['type', 'content', 'title']\n",
    "TYPES = ['fake', 'satire', 'bias', 'conspiracy', 'state', 'junksci', 'hate', 'clickbait', 'unreliable', 'political', 'reliable', 'unknown', 'rumor']\n",
    "#MAPPED_TYPES = ['fake', 'unreliable', 'reliable']\n",
    "MAPPED_TYPES = ['fake', 'reliable']\n",
    "\n",
    "#TYPE_MAPPING = {\n",
    "#    'fake': 'fake',\n",
    "#    'satire': 'fake',\n",
    "#    'bias': 'unreliable',\n",
    "#    'conspiracy': 'unreliable',\n",
    "#    'state': 'fake',\n",
    "#    'junksci': 'fake',\n",
    "#    'hate': 'unreliable',\n",
    "#    'clickbait': 'unreliable',\n",
    "#    'unreliable': 'unreliable',\n",
    "#    'political': 'reliable',\n",
    "#    'reliable': 'reliable',\n",
    "#    'unknown': 'reliable',\n",
    "#    'rumor': 'unreliable'\n",
    "#}\n",
    "\n",
    "TYPE_MAPPING = {\n",
    "    'fake': 'fake',\n",
    "    'satire': 'fake',\n",
    "    'bias': 'fake',\n",
    "    'conspiracy': 'fake',\n",
    "    'state': 'fake',\n",
    "    'junksci': 'fake',\n",
    "    'hate': 'fake',\n",
    "    'clickbait': 'fake',\n",
    "    'unreliable': 'fake',\n",
    "    'political': 'reliable',\n",
    "    'reliable': 'reliable',\n",
    "    'unknown': 'reliable',\n",
    "    'rumor': 'fake'\n",
    "}\n",
    "\n",
    "CLASS_WEIGHTS = {\n",
    "    0: 0.75,\n",
    "    1: 0.25,\n",
    "    2: 0.75\n",
    "}\n",
    "\n",
    "TYPE_INDEX_MAPPING = {}\n",
    "for t, m in TYPE_MAPPING.items():\n",
    "  TYPE_INDEX_MAPPING[t] = MAPPED_TYPES.index(m)\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_WORDS = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def show_batch(dataset):\n",
    "  for batch, labels in dataset.take(1):\n",
    "    for key, value in batch.items():\n",
    "      print(\"{:20s}: {}\".format(key,value.numpy()))\n",
    "    print(\"Types: {}\".format(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"news_cleaned_2018_02_13.csv\"):\n",
    "    !wget https://storage.googleapis.com/researchably-fake-news-recognition/news_cleaned_2018_02_13.csv.zip\n",
    "    !unzip news_cleaned_2018_02_13.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = 'news_cleaned_2018_02_13.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/data/experimental/ops/readers.py:521: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/data/experimental/ops/readers.py:215: shuffle_and_repeat (from tensorflow.python.data.experimental.ops.shuffle_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.shuffle(buffer_size, seed)` followed by `tf.data.Dataset.repeat(count)`. Static tf.data optimizations will take care of using the fused implementation.\n"
     ]
    }
   ],
   "source": [
    "# Dataset found at: https://github.com/several27/FakeNewsCorpus\n",
    "# Dataset file located at: /gdrive/My Drive/news_cleaned_2018_02_13.csv\n",
    "\n",
    "#DATASET_PATH = '/gdrive/My Drive/news_cleaned_2018_02_13.csv'\n",
    "dataset = tf.data.experimental.make_csv_dataset(file_pattern=DATASET_PATH, batch_size=BATCH_SIZE, select_columns=SELECT_COLUMNS, label_name='type', ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to setup an embedding layer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_FILE_PATH = 'glove.6B.{}d.txt'.format(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(GLOVE_FILE_PATH):\n",
    "    !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "    !unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and parse the GloVe embeddings\n",
    "#GLOVE_FILE_PATH = '/gdrive/My Drive/glove.6B.100d.txt'\n",
    "glove_lookup_dict = {}\n",
    "with open(GLOVE_FILE_PATH, 'r') as glove_f:\n",
    "  index = 0\n",
    "  for line in glove_f:\n",
    "      values = line.split()\n",
    "      word = values[0]\n",
    "      if word.isalpha():\n",
    "        if word not in stopwords.words('english'):\n",
    "          coefs = np.asarray(values[1:], dtype='float32')\n",
    "          glove_lookup_dict[word] = (index, coefs)\n",
    "          index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the weighting matrix\n",
    "embedding_matrix = np.zeros((len(glove_lookup_dict) + 1, EMBEDDING_DIM))\n",
    "for i, embedding_vector in glove_lookup_dict.values():\n",
    "  embedding_matrix[i+1] = embedding_vector\n",
    "\n",
    "# Make default value average?\n",
    "#embedding_matrix[0] = np.average(embedding_matrix[1:], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary translation hash\n",
    "keys_tensor = tf.constant(list(glove_lookup_dict.keys()))\n",
    "vals_tensor = tf.constant(list(map(lambda v: v[0], list(glove_lookup_dict.values()))))\n",
    "vocab_table = tf.lookup.StaticHashTable(\n",
    "    tf.lookup.KeyValueTensorInitializer(keys_tensor, vals_tensor), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label number hash\n",
    "labels_tensor = tf.constant(TYPES)\n",
    "label_numbers_tensor = tf.range(len(TYPES))\n",
    "labels_table = tf.lookup.StaticHashTable(\n",
    "    tf.lookup.KeyValueTensorInitializer(labels_tensor, label_numbers_tensor), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapped label number hash\n",
    "labels_to_map_tensor = tf.constant(list(TYPE_INDEX_MAPPING.keys()))\n",
    "mapped_indexes_tensor = tf.constant(list(TYPE_INDEX_MAPPING.values()))\n",
    "mapped_labels_table = tf.lookup.StaticHashTable(\n",
    "    tf.lookup.KeyValueTensorInitializer(labels_to_map_tensor, mapped_indexes_tensor), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup the dataset, map to word indexes\n",
    "\n",
    "def clean_text(t):\n",
    "  t_lower = tf.strings.lower(t)\n",
    "  t_filtered = tf.strings.regex_replace(t_lower, '[%s]' % re.escape(string.punctuation), '')\n",
    "  t_split = tf.strings.split(t_filtered, maxsplit=MAX_WORDS-1)\n",
    "  t_tokenized = vocab_table.lookup(t_split)\n",
    "  return t_tokenized\n",
    "\n",
    "def clean_and_tokenize_dataset(features, labels):\n",
    "  combined_tensor = features['title'] + tf.constant(np.full(features['title'].shape, b' ')) + features['content'] + tf.constant(np.full(features['title'].shape, MAX_WORDS*b' b'))\n",
    "  features = tf.map_fn(clean_text, combined_tensor, dtype=np.int32)\n",
    "  #mapped_labels = tf.one_hot(labels_table.lookup(labels), len(TYPES))\n",
    "  mapped_labels = tf.one_hot(mapped_labels_table.lookup(labels), len(MAPPED_TYPES))\n",
    "  #mapped_labels = mapped_labels_table.lookup(labels)\n",
    "  return features, mapped_labels\n",
    "\n",
    "cleaned_dataset = dataset.map(clean_and_tokenize_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset!\n",
    "def is_train(i, d):\n",
    "  return i % 8 != 0 and i % 8 != 1\n",
    "\n",
    "def is_test(i, d):\n",
    "  return i % 8 == 0\n",
    "\n",
    "def is_val(i, d):\n",
    "  return i % 8 == 1\n",
    "\n",
    "def remove_enumerate(i, d):\n",
    "  return d\n",
    "\n",
    "train_dataset = cleaned_dataset.enumerate().filter(is_train).map(remove_enumerate)\n",
    "test_dataset = cleaned_dataset.enumerate().filter(is_test).map(remove_enumerate)\n",
    "val_dataset = cleaned_dataset.enumerate().filter(is_val).map(remove_enumerate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = keras.layers.Embedding(embedding_matrix.shape[0], EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_WORDS, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do the model a different way...\n",
    "#inp = tf.keras.layers.Input(shape=(MAX_WORDS, EMBEDDING_DIM,))\n",
    "\n",
    "#filter_sizes = [1, 2, 3, 5]\n",
    "#pool_sizes = [5, 3, 2]\n",
    "#num_filters = 256\n",
    "\n",
    "#maxpool_pool = []\n",
    "#for f_size in filter_sizes:\n",
    "  #conv = tf.keras.layers.Conv1D(num_filters, f_size, activation='relu')(inp)\n",
    "  #for p_size in pool_sizes:\n",
    "    #conv = tf.keras.layers.MaxPooling1D(pool_size=p_size)(conv)\n",
    "  #conv = tf.keras.layers.Flatten()(conv)\n",
    "  #maxpool_pool.append(conv)\n",
    "\n",
    "#outp = tf.keras.layers.Concatenate(axis=1)(maxpool_pool)\n",
    "#outp = tf.keras.layers.Flatten()(outp)\n",
    "\n",
    "#conv_model = tf.keras.Model(inputs=inp, outputs=outp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = keras.models.Sequential()\n",
    "#model.add(embedding_layer)\n",
    "#model.add(conv_model)\n",
    "#model.add(keras.layers.Dense(128, activation='relu'))\n",
    "#model.add(keras.layers.Dropout(0.2))\n",
    "#model.add(keras.layers.Dense(64, activation='relu'))\n",
    "#model.add(keras.layers.Dropout(0.2))\n",
    "#model.add(keras.layers.Dense(len(MAPPED_TYPES), activation='softmax'))\n",
    "#model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 128, 300)          98082900  \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 128, 300, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 126, 1, 128)       115328    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 126, 1, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 111, 1, 64)        131136    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 111, 1, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 55, 1, 64)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 55, 1, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3520)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 3520)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              3605504   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 102,205,558\n",
      "Trainable params: 4,122,658\n",
      "Non-trainable params: 98,082,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# From https://link.springer.com/chapter/10.1007/978-3-030-03928-8_17\n",
    "# New model!\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(keras.layers.Reshape((MAX_WORDS, EMBEDDING_DIM, 1)))\n",
    "model.add(keras.layers.Conv2D(filters=128, kernel_size=(3, EMBEDDING_DIM), strides=(1, 1), data_format='channels_last', activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.Conv2D(filters=64, kernel_size=(16, 1), strides=(1, 1), activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 1)))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.Dense(1024, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.Dense(256, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.Dense(32, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.Dense(len(MAPPED_TYPES), activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch callback\n",
    "MODEL_NAME = 'DATAVISMODEL13PAPERBASED'\n",
    "CHECKPOINT_DIR = 'datavismodels/checkpoints/'\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=CHECKPOINT_DIR + MODEL_NAME + '-{epoch:02d}-{val_accuracy:.2f}.hdf5',\n",
    "                                                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 55130 steps, validate for 9188 steps\n",
      "Epoch 1/10\n",
      "55129/55130 [============================>.] - ETA: 0s - loss: 1933253.4213 - accuracy: 0.6994\n",
      "Epoch 00001: saving model to datavismodels/checkpoints/DATAVISMODEL13PAPERBASED-01-0.55.hdf5\n",
      "55130/55130 [==============================] - 21984s 399ms/step - loss: 1933231.6545 - accuracy: 0.6994 - val_loss: 366123.5361 - val_accuracy: 0.5456\n",
      "Epoch 2/10\n",
      "55129/55130 [============================>.] - ETA: 0s - loss: 10783384.1601 - accuracy: 0.7220\n",
      "Epoch 00002: saving model to datavismodels/checkpoints/DATAVISMODEL13PAPERBASED-02-0.55.hdf5\n",
      "55130/55130 [==============================] - 22418s 407ms/step - loss: 10783356.3238 - accuracy: 0.7220 - val_loss: 9857521.3169 - val_accuracy: 0.5456\n",
      "Epoch 3/10\n",
      "55129/55130 [============================>.] - ETA: 0s - loss: 107261834.3623 - accuracy: 0.7149\n",
      "Epoch 00003: saving model to datavismodels/checkpoints/DATAVISMODEL13PAPERBASED-03-0.55.hdf5\n",
      "55130/55130 [==============================] - 22664s 411ms/step - loss: 107263019.9235 - accuracy: 0.7149 - val_loss: 106159137.1519 - val_accuracy: 0.5456\n",
      "Epoch 4/10\n",
      "10305/55130 [====>.........................] - ETA: 4:23:17 - loss: 314984028.6121 - accuracy: 0.6740"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "EPOCHS = 10\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=(TRAINING_SIZE // BATCH_SIZE),\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=(VALIDATION_SIZE // BATCH_SIZE),\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[cp_callback]\n",
    "    #class_weight=CLASS_WEIGHTS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = test_dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predicted labels:\")\n",
    "pred_labels = np.argmax(model.predict(test_set), axis=1)\n",
    "pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Actual labels:\")\n",
    "(k, v) = next(iter(test_set))\n",
    "actual_labels = np.argmax(v, axis=1)\n",
    "actual_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test Set Accuracy: {}\".format(np.sum((pred_labels == actual_labels) / pred_labels.size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
